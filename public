import json

# 读取原始 JSON 文件
with open('your_file.json', 'r') as file:
    data = json.load(file)

# 提取仅包含 key 为 "logits" 的部分
if "logits" in data:
    result = {"logits": data["logits"]}
else:
    result = {}

# 将结果保存为新的 JSON 文件
with open('filtered_file.json', 'w') as outfile:
    json.dump(result, outfile, indent=4)

print("Filtered JSON saved.")

from transformers import AutoModelForCausalLM

from transformers import AutoModelForCausalLM

# 加载模型
model = AutoModelForCausalLM.from_pretrained(args.model)

# 假设要复制的层是第32层（索引31）
src_layer_idx = 31
src_layer = model.model.layers[src_layer_idx]

# 确定新层的索引
new_layer_idx = len(model.model.layers)  # 新层的索引是当前最后一层的索引加1

# 创建新层并传递 layer_idx 参数
new_layer = type(src_layer)(model.config, layer_idx=new_layer_idx)  # 传递 layer_idx

# 复制源层参数到新层
new_layer.load_state_dict(src_layer.state_dict())

# 添加新层到末尾
model.model.layers.append(new_layer)

# 更新模型配置中的层数
model.config.num_hidden_layers = len(model.model.layers)

# 验证新增的层
print(f"总层数: {len(model.model.layers)}")
print(f"最后一层参数是否独立: {id(new_layer) != id(src_layer)}")  # 应输出True

### 多次copy
num_copies = 3
for _ in range(num_copies):
    new_layer = type(src_layer)(model.config)
    new_layer.load_state_dict(src_layer.state_dict())
    model.model.layers.append(new_layer)
model.config.num_hidden_layers += num_copies

        if layers_probs:
            layers_probs = torch.stack(layers_probs, dim=0)  # (num_layers, batch_size, 4)
            num_layers = layers_probs.size(0)

            # 初始化JS散度矩阵
            js_matrix = torch.zeros((num_layers, num_layers), device=device)

            # 计算所有层对之间的JS散度
            for i in range(num_layers):
                for j in range(num_layers):
                    js_matrix[i, j] = js_divergence(layers_probs[i], layers_probs[j]).mean()

            # 累积到总矩阵
            if total_js_matrix is None:
                total_js_matrix = js_matrix.cpu()
            else:
                total_js_matrix += js_matrix.cpu()
            total_samples += batch_size

        # 原有的准确率、熵等计算
        # ... [原有代码] ...

# 计算平均JS散度矩阵并生成热力图
if total_js_matrix is not None and total_samples > 0:
    avg_js_matrix = total_js_matrix / total_samples
    num_layers = avg_js_matrix.size(0)
    layer_labels = [f"Layer {16 + i}" for i in range(num_layers)]

    plt.figure(figsize=(12, 10))
    sns.heatmap(
        avg_js_matrix.numpy(),
        annot=True,
        fmt=".4f",
        xticklabels=layer_labels,
        yticklabels=layer_labels,
        cmap="coolwarm",
        cbar_kws={'label': 'Average JS Divergence'}
    )
    plt.title("JS Divergence Between Layers (Starting from Layer 16)")
    plt.xlabel("Target Layer (j)")
    plt.ylabel("Source Layer (i)")
    plt.tight_layout()
    plt.savefig("js_heatmap_layer16_onwards.png")
    plt.close()
else:
    print("No layers beyond 16 were processed for JS divergence calculation.")

###nltk
现今磷:
以下是实现所需功能的Python函数：

```python
import torch
from nltk import pos_tag
import string

def process_logits(tensor, tokens):
    # 处理标点符号判断
    punctuation = set(string.punctuation)
    
    def is_punctuation(tok):
        return all(c in punctuation for c in tok)
    
    # 第一步：组合tokens成单词并记录对应列索引
    current_word = None
    current_columns = []
    words = []
    columns_list = []
    
    for i, token in enumerate(tokens):
        if is_punctuation(token):
            continue
        
        if token.startswith('▁'):
            if current_word is not None:
                words.append(current_word)
                columns_list.append(current_columns)
            current_word = token.lstrip('▁')
            current_columns = [i]
        else:
            if current_word is not None:
                current_word += token
                current_columns.append(i)
            else:
                current_word = token
                current_columns = [i]
    
    # 添加最后一个单词
    if current_word is not None:
        words.append(current_word)
        columns_list.append(current_columns)
    
    if not words:
        return None
    
    # 第二步：处理tensor生成新矩阵
    processed_columns = []
    for cols in columns_list:
        summed = tensor[:, cols].sum(dim=1, keepdim=True)
        processed_columns.append(summed)
    
    new_tensor = torch.cat(processed_columns, dim=1)
    
    # 第三步：词性标注并筛选名词
    tagged = pos_tag(words)
    noun_indices = [i for i, (_, tag) in enumerate(tagged) if tag.startswith('N')]
    
    if not noun_indices:
        return None
    
    # 构造最终结果
    filtered_tensor = new_tensor[:, noun_indices]
    filtered_words = [words[i] for i in noun_indices]
    
    return filtered_tensor, filtered_words
```

4. **依赖项**：
   - PyTorch
   - NLTK（需要提前下载语料库：`nltk.download('averaged_perceptron_tagger')`）

### 示例调用：
```python
# 示例数据
tokens = ['▁m', 'R', 'NA', ',', '▁t', 'R', 'NA', '▁and', '▁rib', 'os', 'om', 'es', '.']
tensor = torch.randn(32, len(tokens))  # 示例tensor

result = process_logits(tensor, tokens)
if result:
    filtered_tensor, filtered_words = result
    print("Filtered Words:", filtered_words)
    print("Filtered Tensor Shape:", filtered_tensor.shape)
else:
    print("No nouns found")
```

# 在循环外部创建画布和子图
fig, axes = plt.subplots(2, 2, figsize=(24, 20))  # 2x2的画布
axes = axes.flatten()  # 将二维数组转换为一维数组方便索引
plt.suptitle(f"Logits Lens for Question {b_idx}", y=1.02)  # 总标题

for i in range(len(option)):
    # ... 保持原有的数据处理逻辑不变 ...
    
    # 修改绘图部分开始
    ax = axes[i]  # 获取对应的子图
    
    # 保持原有的热图生成逻辑，增加ax参数
    sns.heatmap(
        layer_prob.cpu().detach().numpy(),
        annot=True,
        fmt=".4f",
        xticklabels=token_labels,
        yticklabels=layer_labels,
        cmap="coolwarm",
        vmin=0,
        vmax=1,
        cbar_kws={'label': 'Logits'},
        ax=ax  # 关键修改：指定子图位置
    )
    
    # 子图装饰
    ax.xaxis.tick_top()
    ax.xaxis.set_label_position('top')
    ax.set_title(f"Option {chr(65+i)} ({res})", pad=20)  # 用ABCD表示选项
    ax.set_xlabel("")
    ax.set_ylabel("Layer")

# 调整全局布局并保存
plt.tight_layout()
if not os.path.exists(f"/root/zhujingze/mmlu/res/lens/{args.subject}"):
    os.makedirs(f"/root/zhujingze/mmlu/res/lens/{args.subject}")
plt.savefig(f"/root/zhujingze/mmlu/res/lens/{args.subject}/{args.subject}_Q{b_idx}_all.png")
plt.close()


###attn
import matplotlib.pyplot as plt

def visualize_attention(attention_matrix, tokens):
    plt.figure(figsize=(10, 10))
    plt.imshow(attention_matrix, cmap='viridis')
    plt.xticks(range(len(tokens)), tokens, rotation=90)
    plt.yticks(range(len(tokens)), tokens)
    plt.colorbar()
    plt.show()

# 示例用法
tokens = tokenizer.convert_ids_to_tokens(input_ids_sample[0])
visualize_attention(single_head_attn.detach().numpy(), tokens)

# 查看当前token对前序token的关注度
current_token_idx = -1  # 最后一个token
attention_to_prefix = single_head_attn[current_token_idx, :prefix_len]

# 计算所有头的平均注意力
mean_attn = last_layer_attn.mean(dim=1)[0]  # [seq_len, seq_len]

# 提取特定头的注意力（示例取第一个头）
    head_idx = 0
    single_head_attn = last_layer_attn[0, head_idx]  # [seq_len, seq_len]
###
import string
from nltk import pos_tag

def process_logits(tokens):
    punctuation = set(string.punctuation)

    def is_punctuation(tok):
        return all(c in punctuation for c in tok)

    # 组合tokens成单词并记录对应列索引
    current_word = None
    current_columns = []
    words = []
    columns_list = []

    for i, token in enumerate(tokens):
        if is_punctuation(token):
            continue  # 跳过标点符号

        if token.startswith('▁'):
            # 新单词开始
            if current_word is not None:
                words.append(current_word)
                columns_list.append(current_columns)
            current_word = token.lstrip('▁')  # 去掉前缀
            current_columns = [i]  # 记录当前token的索引
        else:
            # 继续当前单词
            if current_word is not None:
                current_word += token
                current_columns.append(i)
            else:
                # 处理没有前缀的特殊情况（如首个token）
                current_word = token
                current_columns = [i]

    # 添加最后一个单词
    if current_word is not None:
        words.append(current_word)
        columns_list.append(current_columns)

    if not words:
        return []

    # 词性标注并筛选名词
    tagged = pos_tag(words)
    noun_indices = [i for i, (_, tag) in enumerate(tagged) if tag.startswith('N')]

    # 提取每个名词对应的第一个token的索引
    first_token_indices = [columns_list[i][0] for i in noun_indices]

    return first_token_indices

###import csv
import re

def calculate_difference(option1, option2):
    """计算两个选项之间的单词差异数（忽略大小写和标点）"""
    def preprocess(text):
        text = re.sub(r'[^\w\s]', '', text.lower())
        return text.split()
    
    words1 = preprocess(option1)
    words2 = preprocess(option2)
    
    len1, len2 = len(words1), len(words2)
    min_len = min(len1, len2)
    
    # 计算共同长度内的不同单词数
    diff = sum(1 for i in range(min_len) if words1[i] != words2[i])
    # 加上长度差异
    diff += abs(len1 - len2)
    return diff

def filter_csv(input_file, output_file):
    """筛选符合条件的CSV数据"""
    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \
         open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        
        # 读取并写入标题行
        header = next(reader)
        writer.writerow(header)
        
        for row in reader:
            # 假设列顺序为：问题，A，B，C，D，答案
            if len(row) < 6:
                continue  # 跳过不完整的行
            
            question = row[0]
            options = row[1:5]  # A, B, C, D
            answer = row[5].upper().strip()  # 答案字母
            
            # 确定正确选项的索引（0对应A，1对应B，依此类推）
            answer_index = ord(answer) - ord('A')
            if answer_index < 0 or answer_index >= 4:
                continue  # 无效答案，跳过
            
            correct_option = options[answer_index]
            other_indices = [i for i in range(4) if i != answer_index]
            other_options = [options[i] for i in other_indices]
            
            # 检查所有其他选项与正确答案的差异
            valid = True
            for opt in other_options:
                diff = calculate_difference(correct_option, opt)
                if diff > 2:
                    valid = False
                    break
            
            if valid:
                writer.writerow(row)

# 示例用法
input_filename = 'input.csv'  # 输入的CSV文件名
output_filename = 'output.csv'  # 输出的CSV文件名
filter_csv(input_filename, output_filename)

plt.figure(figsize=(10, 4))

# 将 Tensor 转换为 NumPy 数组并展平
np_data = normalized_data.squeeze().cpu().numpy()  # 处理设备及维度

# 绘制折线图
plt.plot(np_data, marker='o', linestyle='-', color='b', label='Normalized Value')

# 标注极值点
plt.scatter(np.argmin(np_data), np.min(np_data), color='r', s=100, label=f'Min ({np.min(np_data):.2f})')
plt.scatter(np.argmax(np_data), np.max(np_data), color='g', s=100, label=f'Max ({np.max(np_data):.2f})')

# 添加标签和标题
plt.title("Normalized Amplitude Variation (Range: 0 to 1)")
plt.xlabel("Data Index")
plt.ylabel("Normalized Value")
plt.ylim(-0.1, 1.1)  # 扩展 y 轴范围以便观察边界值
plt.grid(linestyle='--', alpha=0.6)
plt.legend()
plt.show()
