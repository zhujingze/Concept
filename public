import json

# 读取原始 JSON 文件
with open('your_file.json', 'r') as file:
    data = json.load(file)

# 提取仅包含 key 为 "logits" 的部分
if "logits" in data:
    result = {"logits": data["logits"]}
else:
    result = {}

# 将结果保存为新的 JSON 文件
with open('filtered_file.json', 'w') as outfile:
    json.dump(result, outfile, indent=4)

print("Filtered JSON saved.")

from transformers import AutoModelForCausalLM

from transformers import AutoModelForCausalLM

# 加载模型
model = AutoModelForCausalLM.from_pretrained(args.model)

# 假设要复制的层是第32层（索引31）
src_layer_idx = 31
src_layer = model.model.layers[src_layer_idx]

# 确定新层的索引
new_layer_idx = len(model.model.layers)  # 新层的索引是当前最后一层的索引加1

# 创建新层并传递 layer_idx 参数
new_layer = type(src_layer)(model.config, layer_idx=new_layer_idx)  # 传递 layer_idx

# 复制源层参数到新层
new_layer.load_state_dict(src_layer.state_dict())

# 添加新层到末尾
model.model.layers.append(new_layer)

# 更新模型配置中的层数
model.config.num_hidden_layers = len(model.model.layers)

# 验证新增的层
print(f"总层数: {len(model.model.layers)}")
print(f"最后一层参数是否独立: {id(new_layer) != id(src_layer)}")  # 应输出True

### 多次copy
num_copies = 3
for _ in range(num_copies):
    new_layer = type(src_layer)(model.config)
    new_layer.load_state_dict(src_layer.state_dict())
    model.model.layers.append(new_layer)
model.config.num_hidden_layers += num_copies

        if layers_probs:
            layers_probs = torch.stack(layers_probs, dim=0)  # (num_layers, batch_size, 4)
            num_layers = layers_probs.size(0)

            # 初始化JS散度矩阵
            js_matrix = torch.zeros((num_layers, num_layers), device=device)

            # 计算所有层对之间的JS散度
            for i in range(num_layers):
                for j in range(num_layers):
                    js_matrix[i, j] = js_divergence(layers_probs[i], layers_probs[j]).mean()

            # 累积到总矩阵
            if total_js_matrix is None:
                total_js_matrix = js_matrix.cpu()
            else:
                total_js_matrix += js_matrix.cpu()
            total_samples += batch_size

        # 原有的准确率、熵等计算
        # ... [原有代码] ...

# 计算平均JS散度矩阵并生成热力图
if total_js_matrix is not None and total_samples > 0:
    avg_js_matrix = total_js_matrix / total_samples
    num_layers = avg_js_matrix.size(0)
    layer_labels = [f"Layer {16 + i}" for i in range(num_layers)]

    plt.figure(figsize=(12, 10))
    sns.heatmap(
        avg_js_matrix.numpy(),
        annot=True,
        fmt=".4f",
        xticklabels=layer_labels,
        yticklabels=layer_labels,
        cmap="coolwarm",
        cbar_kws={'label': 'Average JS Divergence'}
    )
    plt.title("JS Divergence Between Layers (Starting from Layer 16)")
    plt.xlabel("Target Layer (j)")
    plt.ylabel("Source Layer (i)")
    plt.tight_layout()
    plt.savefig("js_heatmap_layer16_onwards.png")
    plt.close()
else:
    print("No layers beyond 16 were processed for JS divergence calculation.")

###nltk
现今磷:
以下是实现所需功能的Python函数：

```python
import torch
from nltk import pos_tag
import string

def process_logits(tensor, tokens):
    # 处理标点符号判断
    punctuation = set(string.punctuation)
    
    def is_punctuation(tok):
        return all(c in punctuation for c in tok)
    
    # 第一步：组合tokens成单词并记录对应列索引
    current_word = None
    current_columns = []
    words = []
    columns_list = []
    
    for i, token in enumerate(tokens):
        if is_punctuation(token):
            continue
        
        if token.startswith('▁'):
            if current_word is not None:
                words.append(current_word)
                columns_list.append(current_columns)
            current_word = token.lstrip('▁')
            current_columns = [i]
        else:
            if current_word is not None:
                current_word += token
                current_columns.append(i)
            else:
                current_word = token
                current_columns = [i]
    
    # 添加最后一个单词
    if current_word is not None:
        words.append(current_word)
        columns_list.append(current_columns)
    
    if not words:
        return None
    
    # 第二步：处理tensor生成新矩阵
    processed_columns = []
    for cols in columns_list:
        summed = tensor[:, cols].sum(dim=1, keepdim=True)
        processed_columns.append(summed)
    
    new_tensor = torch.cat(processed_columns, dim=1)
    
    # 第三步：词性标注并筛选名词
    tagged = pos_tag(words)
    noun_indices = [i for i, (_, tag) in enumerate(tagged) if tag.startswith('N')]
    
    if not noun_indices:
        return None
    
    # 构造最终结果
    filtered_tensor = new_tensor[:, noun_indices]
    filtered_words = [words[i] for i in noun_indices]
    
    return filtered_tensor, filtered_words
```

4. **依赖项**：
   - PyTorch
   - NLTK（需要提前下载语料库：`nltk.download('averaged_perceptron_tagger')`）

### 示例调用：
```python
# 示例数据
tokens = ['▁m', 'R', 'NA', ',', '▁t', 'R', 'NA', '▁and', '▁rib', 'os', 'om', 'es', '.']
tensor = torch.randn(32, len(tokens))  # 示例tensor

result = process_logits(tensor, tokens)
if result:
    filtered_tensor, filtered_words = result
    print("Filtered Words:", filtered_words)
    print("Filtered Tensor Shape:", filtered_tensor.shape)
else:
    print("No nouns found")
```

# 在循环外部创建画布和子图
fig, axes = plt.subplots(2, 2, figsize=(24, 20))  # 2x2的画布
axes = axes.flatten()  # 将二维数组转换为一维数组方便索引
plt.suptitle(f"Logits Lens for Question {b_idx}", y=1.02)  # 总标题

for i in range(len(option)):
    # ... 保持原有的数据处理逻辑不变 ...
    
    # 修改绘图部分开始
    ax = axes[i]  # 获取对应的子图
    
    # 保持原有的热图生成逻辑，增加ax参数
    sns.heatmap(
        layer_prob.cpu().detach().numpy(),
        annot=True,
        fmt=".4f",
        xticklabels=token_labels,
        yticklabels=layer_labels,
        cmap="coolwarm",
        vmin=0,
        vmax=1,
        cbar_kws={'label': 'Logits'},
        ax=ax  # 关键修改：指定子图位置
    )
    
    # 子图装饰
    ax.xaxis.tick_top()
    ax.xaxis.set_label_position('top')
    ax.set_title(f"Option {chr(65+i)} ({res})", pad=20)  # 用ABCD表示选项
    ax.set_xlabel("")
    ax.set_ylabel("Layer")

# 调整全局布局并保存
plt.tight_layout()
if not os.path.exists(f"/root/zhujingze/mmlu/res/lens/{args.subject}"):
    os.makedirs(f"/root/zhujingze/mmlu/res/lens/{args.subject}")
plt.savefig(f"/root/zhujingze/mmlu/res/lens/{args.subject}/{args.subject}_Q{b_idx}_all.png")
plt.close()


###attn
import matplotlib.pyplot as plt

def visualize_attention(attention_matrix, tokens):
    plt.figure(figsize=(10, 10))
    plt.imshow(attention_matrix, cmap='viridis')
    plt.xticks(range(len(tokens)), tokens, rotation=90)
    plt.yticks(range(len(tokens)), tokens)
    plt.colorbar()
    plt.show()

# 示例用法
tokens = tokenizer.convert_ids_to_tokens(input_ids_sample[0])
visualize_attention(single_head_attn.detach().numpy(), tokens)

# 查看当前token对前序token的关注度
current_token_idx = -1  # 最后一个token
attention_to_prefix = single_head_attn[current_token_idx, :prefix_len]

# 计算所有头的平均注意力
mean_attn = last_layer_attn.mean(dim=1)[0]  # [seq_len, seq_len]

# 提取特定头的注意力（示例取第一个头）
    head_idx = 0
    single_head_attn = last_layer_attn[0, head_idx]  # [seq_len, seq_len]
###
import string
from nltk import pos_tag

def process_logits(tokens):
    punctuation = set(string.punctuation)

    def is_punctuation(tok):
        return all(c in punctuation for c in tok)

    # 组合tokens成单词并记录对应列索引
    current_word = None
    current_columns = []
    words = []
    columns_list = []

    for i, token in enumerate(tokens):
        if is_punctuation(token):
            continue  # 跳过标点符号

        if token.startswith('▁'):
            # 新单词开始
            if current_word is not None:
                words.append(current_word)
                columns_list.append(current_columns)
            current_word = token.lstrip('▁')  # 去掉前缀
            current_columns = [i]  # 记录当前token的索引
        else:
            # 继续当前单词
            if current_word is not None:
                current_word += token
                current_columns.append(i)
            else:
                # 处理没有前缀的特殊情况（如首个token）
                current_word = token
                current_columns = [i]

    # 添加最后一个单词
    if current_word is not None:
        words.append(current_word)
        columns_list.append(current_columns)

    if not words:
        return []

    # 词性标注并筛选名词
    tagged = pos_tag(words)
    noun_indices = [i for i, (_, tag) in enumerate(tagged) if tag.startswith('N')]

    # 提取每个名词对应的第一个token的索引
    first_token_indices = [columns_list[i][0] for i in noun_indices]

    return first_token_indices
