#data
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

class QuizDataset(Dataset):
    def __init__(self, csv_file, tokenizer_name, max_length=128):
        # 加载数据
        self.data = []
        with open(csv_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                self.data.append(row)

        # 初始化分词器
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length
        self.label_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        question = row["Question"]
        options = [row["OptionA"], row["OptionB"], row["OptionC"], row["OptionD"]]
        correct_answer = self.label_mapping[row["Answer"]]

        # 将问题和每个选项组合起来，进行分词
        encodings = self.tokenizer(
            [f"{question} {option}" for option in options],
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        # 将标签作为目标
        return {
            "input_ids": encodings["input_ids"],       # shape: (4, max_length)
            "attention_mask": encodings["attention_mask"],  # shape: (4, max_length)
            "label": correct_answer                   # 正确答案的索引
        }

# 假设你的 CSV 文件是 'questions.csv'
csv_file = 'questions.csv'
tokenizer_name = 'bert-base-uncased'

dataset = QuizDataset(csv_file, tokenizer_name)
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

#train
from transformers import BertForMultipleChoice, AdamW
import torch.nn.functional as F

# 初始化模型和优化器
model = BertForMultipleChoice.from_pretrained("bert-base-uncased")
optimizer = AdamW(model.parameters(), lr=1e-5)

# 训练循环
for epoch in range(3):  # 假设训练 3 个 epoch
    model.train()
    for batch in train_dataloader:
        optimizer.zero_grad()

        # 获取输入数据
        input_ids = batch["input_ids"]        # shape: (batch_size, 4, max_length)
        attention_mask = batch["attention_mask"]  # shape: (batch_size, 4, max_length)
        labels = batch["label"]              # shape: (batch_size,)

        # 前向传播
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        # 损失值和反向传播
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        print(f"Loss: {loss.item()}")

import torch
import pandas as pd
from torch.utils.data import Dataset
from transformers import AutoTokenizer

class MathProblemDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=512):
        # 读取 CSV 文件
        self.data = pd.read_csv(csv_file)
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # 获取问题和选项
        question = self.data.iloc[idx, 0]  # 第 1 列是问题
        option_a = self.data.iloc[idx, 1]  # 第 2 列是选项 A
        option_b = self.data.iloc[idx, 2]  # 第 3 列是选项 B
        option_c = self.data.iloc[idx, 3]  # 第 4 列是选项 C
        option_d = self.data.iloc[idx, 4]  # 第 5 列是选项 D
        correct_answer = self.data.iloc[idx, 5]  # 第 6 列是正确答案（A/B/C/D）

        # 合并问题和选项作为输入
        input_text = f"Question: {question} A: {option_a} B: {option_b} C: {option_c} D: {option_d}"
        
        # 对输入文本进行编码
        encoded_input = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors="pt")

        # 将答案转换为整数标签（A -> 0, B -> 1, C -> 2, D -> 3）
        answer_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
        label = answer_map.get(correct_answer, -1)  # 如果答案不在字典里，返回 -1（处理异常）

        # 由于返回的是字典，需要从字典中取出tensor
        return encoded_input['input_ids'].squeeze(0), encoded_input['attention_mask'].squeeze(0), torch.tensor(label)


